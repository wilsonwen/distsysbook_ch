# %chapter_number%. 宏观的分布式系统

>分布式程序设计是在多台计算机上解决问题的艺术。

任何计算机系统都需要完成两个基本的任务

- 存储
- 计算

分布式程序设计是在多台计算机上解决问题的艺术。通常来说，是因为这个问题单机无法解决。

没有人要求你必须使用分布式系统。如果有无限的金钱和无限的研发时间，我们就不需要分布式系统。所有的计算和存储都可以在一个神奇的盒子上完成，一个超级迅速和超级可靠的系统。

但是，很少人有无限的资源。所以，他们要找到现实世界中，找到开销与收益的平衡点。在小规模下，升级硬件是一个可行的策略。不过，随着问题体积变大，你会达到硬件更新无法在单节点下解决问题的临界点。在这种情况下，我热烈欢迎你来到分布式系统的世界。

现实是，最有价值的是中档的商用硬件设备，只要其维护费用可以通过容错软件来降低。

计算主要受益于可以以内存访问替代网络访问的高端硬件。而高端硬件的性能会受限于需要节点间大量通信的任务。

![cost-efficiency](images/barroso_holzle.png)

正如上图所示[Barroso, Clidaras & Holzle](http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024),在统一内存访问的模型下，高端硬件与普通硬件的性能差别随着集群的规模降低。

理想情况下，添加一台机器可以线性的提升系统的性能和容量。不过这当然是不可能的，因为会有额外的开销。数据需要复制多次，计算任务需要协同工作等。这就是学习分布式算法的原因———分布式算法针对特定问题提供了有效的解决方法，同时明确了什么是可能发生的，正确的实现的最小开销是多少和什么是不可能发生的。

本书的重点在于传统的但商业相关的分布式程序设计和分布式系统：数据中心。例如，我不会讨论奇怪的网络设置或者共享内存设置里的问题。另外，我们的重点在与探讨系统的设计空间，而不是优化任何特定的设计，后者是一个更加专业的话题。

## 我们的目的：可扩展性和其他好处

我看待问题的方法，所有事情都需要考虑规模。

大多是事情在小规模时都是微不足道的。但是一旦你超过一定的规模、流量或其他物理限制时，问题就变得困难。拿起一块巧克力很简单，但是拿起一座山就很难。统计一间房间里的人数很简单，统计一个国家的人数就很难。

所以任何事情都伴随着规模 —— 可扩展性。非正式的说，在一个可扩展系统里，我们从小规模到大规模，事情不应该逐渐的恶化。下面是另一个定义：

<dl>
  <dt>[可扩展性](http://zh.wikipedia.org/wiki/可扩展性)</dt>
  <dd>是系统，网络或进程能够在处理能力范围内处理不断增长的负荷的能力，或者是能够扩展适应增长的能力。</dd>
</dl>

不断增长的是什么？你几乎可以从任何角度来衡量增长（人口数量，电力使用等等）。但是，有三个有趣的事情：

- 规模可扩展性：新增计算节点应该可以使系统线性增速；数据的增长不应该增加时延。
- 地理可扩展性：在合理范围内处理跨数据中心的时延，使用多个数据中心可以减短用户查询的响应时间。
- 管理可扩展性：新增计算节点不应该增加系统管理开销。

当然，在现实系统中，增长是同时发生在多个维度的；每一项度量只包含了增长的一些方面。

分布式系统能够持续满足用户的扩展需求。其中有两个特别相关的方面 - 性能与可用性 - 可以用多个方法来衡量。

### 性能 (和时延)

<dl>
  <dt>[性能](http://en.wikipedia.org/wiki/Computer_performance)</dt>
  <dd>由计算机系统使用特定时间与资源可完成的任务来量化</dd>
</dl>

基于此，我们希望达到以下一个或多个目标：

- 短暂的响应时间/低时延
- 高吞吐率（处理任务的速率）
- 资源占用率低

优化其中任一项都需要权衡相互的影响。例如，系统可以通过处理较大的批处理任务，从而减少额外操作，达到更高的吞吐率。折衷是，由于批处理单个任务的响应时间会更长。

我发现低时延 - 短响应时间 - 是性能中最有趣的方面，因为其与物理限制有很强的关联。使用经济资源解决时延问题，比解决其他性能问题更加困难。

时延的定义有很多，其中我最认同的是这个单词创造者的说法：

<dl>
  <dt>时延</dt>
  <dd>延迟的状态；事物启动到发生的间隔</dd>
</dl>

那么延迟意味着什么呢？

<dl>
  <dt>延迟</dt>
  <dd>存在但未其作用</dd>
</dl>

这个定义很cool，因为它强调了事情发生到其作用的间隔时间。

例如，想象一下你被僵尸病毒感染了。时延就是你感染病毒到变成僵尸的时间。这就是时延：事情发生却未可见的时间。

我们假设，分布式系统只处理一件高层次的任务：给定一个任务，系统用所有数据计算出一个结果。换句话说，将分布式系统想成是一个具备特定计算能力的数据存储，计算方法为：

`result = query(all data in the system)`

那么，影响时延的不是旧数据的数量，而是新数据起作用的速度。例如，时延由写操作对读者可见的时间来衡量。

这个定义的另一个关键点是，如何没有事情发生，就没有时延。数据不发生改变的系统，没有时延的问题。

在分布式系统中，有一个最低时延是无法克服的：光速限制了信息的传播速度；硬件操作有最低的时延开销。

最低时延的影响，由查询本身和信息传播的物理距离决定。

### 可用性 (和故障容错性)

可扩展系统的第二个方面就是可用性。

<dl>
  <dt>[可用性](http://en.wikipedia.org/wiki/High_availability)</dt>
  <dd>系统可用的时间比例。如果一个用户不能访问系统，那么该系统即不可用。</dd>
</dl>

分布式系统允许我们实现单机系统很难实现的特性。例如，单机不能容忍任何的故障。

分布式系统由一群不可靠的部分组成，在其之上构建一个可靠的系统。

没有冗余的系统的可用性，与其下层部分一样。基于冗余构建的系统能够容忍分区部分故障，从而更加可用。请注意，冗余在不同的层面意义不一样 - 组件，服务器，数据中心等等。

根据公式，可用性表示为： `Availability = uptime / (uptime + downtime)`。

从技术的角度来看，可用性主要是故障容错的能力。因为故障发生的概率会随着系统部件数量而增长，系统应该能够通过技术手段使可靠性不会随着部件数量而增长。

例如：

<table>
<tr>
  <td>可用性 %</td>
  <td>每年的停机时间是多少</td>
</tr>
<tr>
  <td>90% ("1个9")</td>
  <td>超过一个月</td>
</tr>
<tr>
  <td>99% ("2个9")</td>
  <td>少于4天</td>
</tr>
<tr>
  <td>99.9% ("3个9")</td>
  <td>少于9个小时</td>
</tr>
<tr>
  <td>99.99% ("4个9")</td>
  <td>少于1个小时</td>
</tr>
<tr>
  <td>99.999% ("5个9")</td>
  <td>~ 5分钟</td>
</tr>
<tr>
  <td>99.9999% ("6个9")</td>
  <td>~ 31秒</td>
</tr>
</table>


广义上说可用性不单单是正常运行时间，因为服务的可用行可能会受网络停电或者公司倒闭影响(这虽然与故障容忍不相关，但确实影响了系统的可用性)。但是在不了解系统的每一方面的情况下。我们能做的就是做好故障容错设计。

故障容错意味着什么？

<dl>
  <dt>故障容错</dt>
  <dd>系统在发生故障时正常运行的能力</dd>
</dl>

故障容错归结为：定义预料到的故障，然后为其设计一个系统或者算法。你不能容错考虑不到的故障。

## 什么阻碍了我们达成目标？

分布式系统受限于两个物理因素：

- 计算节点的数量（因为存储和计算容量的要求）
- 计算节点的距离（信息最快以光速传播）

因为这些限制：

- 计算节点的数量增长会导致系统故障的概率增加（降低可用性和增加管理开销）
- 计算节点的数量增长使节点间通信增加（随着规模增大而性能降低）
- 地理距离的增加导致节点间的最小时延变大（特定的操作会降低性能）

这些物理限制引入的变化，就是系统设计世界里的选项。

性能与可用性是由系统外部保证来定义的。在一个高层次上，你可以将这些保证看成是系统的SLA(服务水平协议）：如果写入数据，其他地方访问需要多久？数据写入后，能够得到怎样的持久性保证？如果我向系统发送计算请求，需要多久返回结果？当部件发生故障或者卸下以后，系统会有什么样的影响？

这里有另外一个没有明确提出的评判标准：可理解性。这些协议的可理解性如何？当然，并没有可理解度的简单衡量方法。

我倾向与将可理解性放在物理限制下面。毕竟，人们对理解物理限制的方方面面花费了很多时间[我们只考虑了部分](http://en.wikipedia.org/wiki/Working_memory#Capacity)。错误与异常有区别，错误指的是不正确的行为，而异常是指意想不到的行为。如果你聪明，就要预料到异常会发生。

## 抽象与建模

下面来说说抽象与建模。抽象通过移除真实世界中问题无关的方面，使得一切更加可控。模型用一种简洁的方式描述了分布式系统中的关键属性。我将会在下一章中讨论很多中模型，例如：

- 系统模型（异步/同步）
- 故障模型（崩溃故障，分区，拜占庭）
- 一致性模型（强一致性，最终一致性）

一个好的抽象能够涵盖相关的关注点，使得系统更加容易理解。

现实中我们会有许多的计算节点，与我们期待的像单机系统一样工作存在着落差。通常，最熟悉的模型（例如，实现一个具备共享内存抽象的分布式系统）代价太大。

系统，如果做出稍微弱一点的保证，能够得到更大的自由，从而得到更好的性能。但是也更加难以对其推论。如单机一样工作的系统比多节点系统更加容易进行推论。

通过暴露系统内部更多的细节，可以提升系统的性能。例如，在[列式存储](http://en.wikipedia.org/wiki/Column-oriented_DBMS)，用户可以推理出键值对在系统内的局部性，然后对典型查询的影响做出决定。而隐藏这些细节的系统更加容易理解（因为它们更多的作为一个单体，而无须考虑过多的细节），系统暴露更多的细节性能更好（因为它们贴切的反映了现实）。

许多种类型的故障使得编写像单机一样的分布式系统更加困难。网络时延和网络分区（计算节点间的网络故障）意味着有时需要做出艰难的抉择，是为了保持可用而损失一些重要的保证，还是为了安全而拒绝客户访问。

CAP理论 - 我们下一章会讨论 - 涵盖了上面描述的冲突。最终，理想的系统既满足程序员的需要（简洁的语义）和业务需要（可用性/一致性/时延）。

## 设计技术：分区与副本

数据分布在多个数据节点的原则是非常重要的。需要计算时，我们要定位到数据所在节点。

有两种基础的技术可以运用在数据上。数据切分在多个数据节点（分区）运行进行并行处理。数据也可以复制或者缓存在不同的节点上从而减少服务器到客户端的距离和获得更好的故障容错能力（复制备份）。

> 分而治之 - 我指的是，分区与副本

以下这张图片描述了两者之间的区别：分片的数据（A和B）被划分在两个不同的集合里，而副本数据（C）复制到多个节点。

![分区与副本](images/part-repl.png)

这是用来解决分布式计算遇到任何问题的组合拳。当然，技巧在于选择合适的技术用到具体实现中。有许许多多的算法，实现复制与分区，每一个都有不同的限制与优势，你需要根据设计目标进行评估取舍。

### 分区

分区是将数据集划分到多个小的独立集合。这用来减少数据集增长带来的影响，因为没一个分区都是一个数据的子集。

- 分区通过限制扫描数据的数量和将相关数据分到同一分区来改善性能。
- 分区通过允许各个分区独立失效来提高可用性，在牺牲可用性之前增加节点的数量。

同时，分区与实际应用有很强的关联，所以很难在不清楚具体细节时进行讨论。这就是为什么我更多的会讨论副本。

### 副本

副本即在多个机器上存储同一分数据；这允许更多的服务器参与到计算。

引自 [Homer J. Simpson](http://en.wikipedia.org/wiki/Homer_vs._the_Eighteenth_Amendment):

> 副本！是所有问题的原因和解决方法。

副本 - 复制和重新生成事物 - 是我们与时延做斗争的主要方法。

- 副本通过拷贝的数据利用更多的计算资源和带宽，从而提升性能。
- 副本通过创建数据的拷贝，在牺牲可用性前增加可能出现故障的节点，从而提高可用性。

副本是关于提供额外的带宽，和在高负载的地方提供缓存。也是关于根据一致性模型维护一致性。

副本可是使我们获得可扩展性，性能和故障容错性。害怕失去可用性或者性能降低？复制数据从而避免但点故障。计算太慢？复制数据到多个系统上。IO太慢？复制数据到本地缓存减少时延或者复制到多台机器上增加吞吐率。

副本也是许多问题的根源，因为副本导致多个独立的数据拷贝需要在多个机器间进行同步 - 这意味这要保证副本遵循一定的一致性模型。

一致性模型的选择非常关键：一个好的一致性模型为程序员提供了简洁的语义（换句话说，其提供的保证是非常容易推论的）和满足业务／设计的需求，例如高可用性或者强一致性。

副本的唯一一致性模型 - 强一致性 - 允许像没有副本一样进行编程。另一种一致性模型暴露副本的内部细节给程序员。但是，若一致性可以提供更低的时延和高可用性 - 同时并不会更难于理解，只是不一样。

---

## 更多的阅读材料

- [The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines](http://www.morganclaypool.com/doi/pdf/10.2200/s00193ed1v01y200905cac006) - Barroso &  Hölzle, 2008
- [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing)
- [Notes on Distributed Systems for Young Bloods](http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/) - Hodges, 2013
